# Reference error detection

Report abstract: Abstractive summarization algorithms are prone to generating summaries that are factually inconsistent with respect to the source text. In line with this problem, recent work has introduced automatic metrics for evaluating the factual consistency of summaries. However, the proposed top-down machine-learning-based approaches are limited in the inter-annotator agreement on their validation data and their explainability. In addition, the extend to which these models cover all types of factual errors is current unknown. 

In this work, we propose a bottom-up detection algorithm that focuses on detecting a single type of error to complement these top-down approaches, with the goal of constituting a simple, explainable and reliable module for detecting this error type. The proposed model, based on comparing coreference chains between source and summary text, showed moderate performance on validation data and poor performance on test data. The causes of poor performance were attributed to sub-optimal mention linking between source and summary texts, coreference annotation issues and limitations in the evaluation.
